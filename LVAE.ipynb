{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN1ER9xw5HtG9YxFHNcH6OU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jcandane/SSA/blob/main/LVAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# setup"
      ],
      "metadata": {
        "id": "n5XDuDiKJ5Cj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p LVAE"
      ],
      "metadata": {
        "id": "Ypr-aJUz4j4p"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lc2G7IG93TS1",
        "outputId": "c2bb3800-06a9-4ee4-9a79-f1d81e2ce86b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting LVAE/LVAE.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile LVAE/LVAE.py\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import optax\n",
        "from flax import linen as nn\n",
        "from flax.training import train_state\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from functools import partial\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "from flax import traverse_util\n",
        "from flax.jax_utils import replicate\n",
        "from flax.serialization import to_bytes, from_bytes\n",
        "\n",
        "import json\n",
        "from huggingface_hub import HfApi, HfFolder, Repository, hf_hub_download, upload_file\n",
        "\n",
        "# PCA Encoder (flattened input)\n",
        "class PCAEncoder(nn.Module):\n",
        "    d: int\n",
        "    input_dim: int\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        x     = x.reshape(x.shape[0], -1)\n",
        "        μ     = nn.Dense(self.d)(x)\n",
        "        logσ2 = nn.Dense(self.d)(x)\n",
        "        σ     = jnp.exp(0.5 * logσ2)\n",
        "        ε     = jax.random.normal(self.make_rng('noise'), μ.shape)\n",
        "        return μ + σ * ε, μ, logσ2\n",
        "\n",
        "# PCA Decoder\n",
        "class PCADecoder(nn.Module):\n",
        "    output_shape: tuple\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, z):\n",
        "        flat_dim = np.prod(self.output_shape)\n",
        "        x = nn.Dense(flat_dim)(z)\n",
        "        x = x.reshape(-1, *self.output_shape)\n",
        "        return x\n",
        "\n",
        "# PCA-VAE Model\n",
        "class PCAVAE(nn.Module):\n",
        "    d: int\n",
        "    input_shape: tuple\n",
        "\n",
        "    def setup(self):\n",
        "        input_dim = np.prod(self.input_shape)\n",
        "        self.encoder = PCAEncoder(self.d, input_dim)\n",
        "        self.decoder = PCADecoder(self.input_shape)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        z, μ, logσ2 = self.encoder(x)\n",
        "        recon       = self.decoder(z)\n",
        "        return recon, μ, logσ2\n",
        "\n",
        "# Training wrapper with β and LR scheduling\n",
        "class LVAE:\n",
        "    def __init__(self, d=16, epochs=20, batch_size=128, seed=0, β=1.0,\n",
        "                 modal_id:str=\"\", modal_secret:str=\"\", hf_token:str=\"\", repo_id:str=\"\"):\n",
        "        self.d = d\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.seed = seed\n",
        "        self.rng = jax.random.PRNGKey(seed)\n",
        "        self.losses, self.recons, self.kls = [], [], []\n",
        "        self.β = β\n",
        "\n",
        "        # New cloud integration parameters\n",
        "        self.modal_id     = modal_id\n",
        "        self.modal_secret = modal_secret\n",
        "        self.hf_token     = hf_token\n",
        "        self.repo_id      = repo_id\n",
        "\n",
        "    @property\n",
        "    def config(self):\n",
        "        #exclude = {\"model\", \"state\", \"ds\"} ## exclude these from defining the class\n",
        "        exclude = {\"model\", \"state\", \"ds\", \"rng\", \"losses\", \"recons\", \"kls\", \"verbose\"}\n",
        "        return {k: v for k, v in self.__dict__.items() if k not in exclude}\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        init_keys = cls.__init__.__annotations__.keys()\n",
        "        filtered_config = {k: config[k] for k in init_keys if k in config}\n",
        "        return cls(**filtered_config)\n",
        "\n",
        "    def init_model(self, data):\n",
        "        input_shape = data.shape[1:]\n",
        "        flat_data = data.reshape(len(data), -1)\n",
        "        pca = PCA(n_components=self.d)\n",
        "        pca.fit(flat_data)\n",
        "        pca_weights = pca.components_.T\n",
        "\n",
        "        self.model = PCAVAE(self.d, input_shape)\n",
        "        variables = self.model.init({'params': self.rng, 'noise': self.rng}, jnp.ones((1, *input_shape)))\n",
        "\n",
        "        rng_weights = np.random.RandomState(self.seed).normal(scale=1e-2, size=pca_weights.shape)\n",
        "        variables['params']['encoder']['Dense_0']['kernel'] = (pca_weights+rng_weights).astype(np.float32)\n",
        "        variables['params']['encoder']['Dense_0']['bias']   = np.zeros(self.d, dtype=np.float32)\n",
        "        variables['params']['encoder']['Dense_1']['kernel'] = np.zeros((flat_data.shape[1], self.d), dtype=np.float32)\n",
        "        variables['params']['encoder']['Dense_1']['bias']   = np.full((self.d,), -10.0, dtype=np.float32)\n",
        "\n",
        "        variables['params']['decoder']['Dense_0']['kernel'] = (pca_weights+rng_weights).T.astype(np.float32)\n",
        "        variables['params']['decoder']['Dense_0']['bias']   = flat_data.mean(axis=0).astype(np.float32)\n",
        "\n",
        "        steps_per_epoch = len(data) // self.batch_size\n",
        "        schedule = optax.warmup_cosine_decay_schedule(\n",
        "            init_value=1e-7, peak_value=1e-4,\n",
        "            warmup_steps=steps_per_epoch*2,  # 2 epochs warmup\n",
        "            decay_steps=steps_per_epoch*(self.epochs-2),\n",
        "            end_value=1e-7,\n",
        "        )\n",
        "\n",
        "        self.tx = optax.adam(schedule)\n",
        "        self.state = train_state.TrainState.create(\n",
        "            apply_fn=self.model.apply, params=variables['params'], tx=self.tx\n",
        "        )\n",
        "\n",
        "    @partial(jax.jit, static_argnums=0)\n",
        "    def train_step(self, state, batch, rng, β):\n",
        "        def loss_fn(params):\n",
        "            recon, μ, logσ2 = self.model.apply({'params': params}, batch, rngs={'noise': rng})\n",
        "            recon_loss      = jnp.mean((batch - recon)**2)\n",
        "            kl_loss         = -0.5 * jnp.mean(1 + logσ2 - μ**2 - jnp.exp(logσ2))\n",
        "            return recon_loss + β * kl_loss, (recon_loss, kl_loss)\n",
        "\n",
        "        grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
        "        (loss, (recon_loss, kl_loss)), grads = grad_fn(state.params)\n",
        "        state = state.apply_gradients(grads=grads)\n",
        "        return state, loss, recon_loss, kl_loss\n",
        "\n",
        "    def train(self, dataset, gpu=None):\n",
        "\n",
        "        if gpu is not None:\n",
        "            modal_app_code = f'''\n",
        "import modal\n",
        "\n",
        "jax_image = (\n",
        "    modal.Image.debian_slim()\n",
        "    .run_commands(\"pip install --upgrade pip\")\n",
        "    .run_commands(\n",
        "        \"pip install flax optax einops matplotlib scikit-learn numpy huggingface_hub jax[cuda12] --find-links https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\"\n",
        "    )\n",
        ")\n",
        "\n",
        "app = modal.App(name=\"LVAE-app\")\n",
        "\n",
        "@app.function(image=jax_image, gpu=\"{gpu}\", timeout=400)\n",
        "def train_remote(config, dataset, hf_token=None, repo_id=None):\n",
        "    from LVAE.LVAE import LVAE\n",
        "    tvae = LVAE(**config)\n",
        "    tvae.train(dataset) #, hf_token=hf_token, repo_id=repo_id)\n",
        "    return None'''\n",
        "\n",
        "            # Write to modal_app.py\n",
        "            with open(\"LVAE/modal_app.py\", \"w\") as f:\n",
        "                f.write(modal_app_code)\n",
        "\n",
        "            # Optional remote training with Modal\n",
        "            from LVAE.modal_app import app, train_remote\n",
        "\n",
        "            with app.run():\n",
        "                return train_remote.remote(self.config, dataset) #, hf_token=self.hf_token, repo_id=self.repo_id)\n",
        "\n",
        "\n",
        "        self.init_model(dataset)\n",
        "        num_batches = len(dataset) // self.batch_size\n",
        "\n",
        "        for epoch in range(1, self.epochs + 1):\n",
        "            β = min(self.β, epoch / (self.epochs * 0.25))  # Linear β schedule from 0 to 1 halfway through training\n",
        "            epoch_loss = epoch_recon = epoch_kl = 0.0\n",
        "            perm = np.random.permutation(len(dataset))\n",
        "\n",
        "            for i in range(num_batches):\n",
        "                batch_idx = perm[i*self.batch_size:(i+1)*self.batch_size]\n",
        "                batch = dataset[batch_idx]\n",
        "                self.rng, rng_step = jax.random.split(self.rng)\n",
        "                self.state, loss, r_loss, kl_loss = self.train_step(self.state, batch, rng_step, β)\n",
        "                epoch_loss  += loss\n",
        "                epoch_recon += r_loss\n",
        "                epoch_kl    += kl_loss\n",
        "\n",
        "            avg_loss  = epoch_loss / num_batches\n",
        "            avg_recon = epoch_recon / num_batches\n",
        "            avg_kl    = epoch_kl / num_batches\n",
        "            self.losses.append(avg_loss)\n",
        "            self.recons.append(avg_recon)\n",
        "            self.kls.append(avg_kl)\n",
        "            if epoch % 100 == 0:\n",
        "                print(f\"Epoch {epoch}: Loss={avg_loss:.4f}, Recon={avg_recon:.4f}, KL={avg_kl:.4f}, β={β:.2f}\")\n",
        "\n",
        "        self.upload_to_huggingface(self.repo_id)\n",
        "        self.plot_results(dataset, round=False)\n",
        "\n",
        "    def reconstruct(self, x):\n",
        "        self.rng, rng_step = jax.random.split(self.rng)\n",
        "        recon, _, _ = self.model.apply({'params': self.state.params}, x, rngs={'noise': rng_step})\n",
        "        return recon\n",
        "\n",
        "    def upload_to_huggingface(self,\n",
        "                              repo_id: str,\n",
        "                              weights_file: str = \"flax_model.msgpack\",\n",
        "                              config_file: str = \"config.json\"):\n",
        "\n",
        "        assert self.hf_token, \"🤗 HuggingFace token (`hf_token`) is not set!\"\n",
        "\n",
        "        # Save token to local HfFolder for API authentication\n",
        "        HfFolder.save_token(self.hf_token)\n",
        "        api = HfApi()\n",
        "\n",
        "        # ── 1. Save model parameters ────────────────────────────────────────\n",
        "        with open(weights_file, \"wb\") as f:\n",
        "            f.write(to_bytes(self.state.params))\n",
        "\n",
        "        # ── 2. Merge model & training configuration ─────────────────────────\n",
        "        full_config = {\n",
        "            \"d\": self.d,\n",
        "            \"epochs\": self.epochs,\n",
        "            \"batch_size\": self.batch_size,\n",
        "            \"seed\": self.seed,\n",
        "            \"modal_id\": self.modal_id,\n",
        "            \"modal_secret\": self.modal_secret,\n",
        "            \"rng\": self.rng.tolist() if isinstance(self.rng, jnp.ndarray) else self.rng,\n",
        "            \"training_time\": getattr(self, \"training_time\", None),\n",
        "            \"losses\": [float(x) for x in self.losses],\n",
        "            \"recons\": [float(x) for x in self.recons],\n",
        "            \"kls\": [float(x) for x in self.kls],\n",
        "            \"input_shape\": self.model.input_shape,\n",
        "            \"latent_dim\": self.d,\n",
        "            \"model_type\": \"Linear Variational Autoencoder (LVAE)\"\n",
        "        }\n",
        "\n",
        "        with open(config_file, \"w\") as f:\n",
        "            json.dump(full_config, f, indent=2)\n",
        "\n",
        "        # ── 3. Upload files to HuggingFace ──────────────────────────────────\n",
        "        for fname in [weights_file, config_file]:\n",
        "            upload_file(\n",
        "                path_or_fileobj=fname,\n",
        "                path_in_repo=fname,\n",
        "                repo_type=\"dataset\",\n",
        "                repo_id=repo_id,\n",
        "                token=self.hf_token\n",
        "            )\n",
        "\n",
        "        print(f\"🤗 LVAE successfully uploaded to Hugging Face: {repo_id}\")\n",
        "\n",
        "\n",
        "    def plot_results(self, x, num_images=8, round=False):\n",
        "        recon = self.reconstruct(x[:num_images])\n",
        "        if round:\n",
        "            recon = np.round(recon).astype(bool)\n",
        "        fig, axes = plt.subplots(2, num_images, figsize=(num_images*2,4))\n",
        "        for i in range(num_images):\n",
        "            axes[0,i].imshow(x[i].squeeze(), cmap='gray'); axes[0,i].axis('off')\n",
        "            axes[1,i].imshow(recon[i].squeeze(), cmap='gray'); axes[1,i].axis('off')\n",
        "        mnist_example = \"mnist_example.png\"\n",
        "        plt.savefig(mnist_example)\n",
        "        plt.show()\n",
        "\n",
        "        plt.figure(figsize=(8,4))\n",
        "        plt.plot(self.losses, label='Total loss')\n",
        "        plt.plot(self.recons, label='Recon loss')\n",
        "        plt.plot(self.kls, label='KL loss')\n",
        "        plt.yscale(\"log\")\n",
        "        plt.legend()\n",
        "        plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.show()\n",
        "        loss_png = \"Loss\"+\".png\"\n",
        "        plt.savefig(loss_png)\n",
        "        plt.show()\n",
        "\n",
        "        if self.hf_token!=\"\" and self.repo_id!=\"\":\n",
        "            upload_file(\n",
        "                path_or_fileobj=loss_png,\n",
        "                path_in_repo=loss_png,\n",
        "                repo_type=\"dataset\",\n",
        "                repo_id=self.repo_id,\n",
        "                token=self.hf_token\n",
        "            )\n",
        "            print(f\"🖼️ Visualization uploaded to Hugging Face: {self.repo_id}/{loss_png}\")\n",
        "            upload_file(\n",
        "                path_or_fileobj=mnist_example,\n",
        "                path_in_repo=mnist_example,\n",
        "                repo_type=\"dataset\",\n",
        "                repo_id=self.repo_id,\n",
        "                token=self.hf_token\n",
        "            )\n",
        "            print(f\"🖼️ Visualization uploaded to Hugging Face: {self.repo_id}/{mnist_example}\")\n",
        "\n",
        "def set_modal_tokens(token_id: str, token_secret: str):\n",
        "    import subprocess\n",
        "\n",
        "    cmd = [\n",
        "        \"modal\", \"token\", \"set\",\n",
        "        \"--token-id\", token_id,\n",
        "        \"--token-secret\", token_secret\n",
        "    ]\n",
        "    subprocess.run(cmd, check=True)\n",
        "    print(\"✅ Tokens configured successfully.\")\n",
        "    return None"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get User Login Data"
      ],
      "metadata": {
        "id": "7Xjg5Kfs-J1P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import numpy as np\n",
        "try:\n",
        "    import modal\n",
        "except ImportError:\n",
        "    !pip install -q modal\n",
        "    import modal\n",
        "try:\n",
        "    from huggingface_hub import HfApi, HfFolder, Repository, hf_hub_download, upload_file\n",
        "except:\n",
        "    !pip install -q -U huggingface_hub\n",
        "    from huggingface_hub import HfApi, HfFolder, Repository, hf_hub_download, upload_file\n",
        "from LVAE.LVAE import set_modal_tokens\n",
        "\n",
        "MODAL_ID     = userdata.get(\"MODAL_TOKEN_ID\")\n",
        "MODAL_SECRET = userdata.get(\"MODAL_TOKEN_SECRET\")\n",
        "HF_TOKEN     = userdata.get(\"HF_TOKEN\")\n",
        "HF_REPO      = \"jcandane/pca\" ### <<---- change this!\n",
        "set_modal_tokens(MODAL_ID, MODAL_SECRET)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHPxM3w73lQa",
        "outputId": "cc513bc6-45fd-4aec-e82d-8355253c1297"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Tokens configured successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run Simulation"
      ],
      "metadata": {
        "id": "b0H0HJUrJ8lo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "from LVAE.LVAE import LVAE\n",
        "\n",
        "### MNIST Dataset\n",
        "(x_train, _), _ = mnist.load_data()\n",
        "x_train         = (x_train / 255.).astype(np.float32)[..., None]\n",
        "\n",
        "### LVAE Training\n",
        "lae = LVAE(d=100, epochs=100, batch_size=128, seed=42, β=0.1, hf_token=HF_TOKEN, repo_id=HF_REPO, modal_id=MODAL_ID, modal_secret=MODAL_SECRET) ## d=200\n",
        "lae.train(x_train, gpu=\"h100\")"
      ],
      "metadata": {
        "id": "Rg2LMIvk3cCA"
      },
      "execution_count": 4,
      "outputs": []
    }
  ]
}